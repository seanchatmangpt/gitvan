# Citty Test Utils - Comprehensive Test Suite

A complete testing framework for GitVan CLI with unit, integration, and BDD tests.

## ğŸ§ª Test Structure

```
tests/
â”œâ”€â”€ unit/                    # Unit tests for individual components
â”‚   â”œâ”€â”€ assertions.test.mjs  # Fluent assertion API tests
â”‚   â”œâ”€â”€ scenario-dsl.test.mjs # Scenario DSL and test utils tests
â”‚   â””â”€â”€ local-runner.test.mjs # Local runner component tests
â”œâ”€â”€ integration/             # Integration tests for component interactions
â”‚   â””â”€â”€ full-integration.test.mjs # Cross-component integration tests
â””â”€â”€ bdd/                     # BDD tests with scenario-based testing
    â””â”€â”€ gitvan-cli-bdd.test.mjs # Behavior-driven development tests
```

## ğŸš€ Quick Start

### Run All Tests
```bash
pnpm test:run
```

### Run Specific Test Types
```bash
# Unit tests only
pnpm test:unit

# Integration tests only
pnpm test:integration

# BDD tests only
pnpm test:bdd

# With coverage
pnpm test:coverage
```

### Interactive Testing
```bash
# Watch mode
pnpm test:watch

# UI mode
pnpm test:ui
```

### Comprehensive Test Runner
```bash
node run-tests.mjs
```

## ğŸ“‹ Test Categories

### Unit Tests (`tests/unit/`)

**Purpose**: Test individual components in isolation

**Coverage**:
- âœ… Fluent assertion API (`assertions.test.mjs`)
- âœ… Scenario DSL and test utilities (`scenario-dsl.test.mjs`)
- âœ… Local runner component (`local-runner.test.mjs`)

**Key Features**:
- Mocked dependencies for isolated testing
- Comprehensive assertion method testing
- Error handling validation
- Method chaining verification

### Integration Tests (`tests/integration/`)

**Purpose**: Test component interactions and real CLI execution

**Coverage**:
- âœ… Local runner with actual GitVan CLI
- âœ… Cleanroom runner with Docker containers
- âœ… Scenario DSL execution
- âœ… Test utilities integration
- âœ… Cross-component workflows

**Key Features**:
- Real CLI command execution
- Docker container management
- File system operations
- Multi-step workflows

### BDD Tests (`tests/bdd/`)

**Purpose**: Behavior-driven development with user-focused scenarios

**Coverage**:
- âœ… CLI help system scenarios
- âœ… Error handling scenarios
- âœ… Command-specific help scenarios
- âœ… Docker cleanroom scenarios
- âœ… Complex workflow scenarios
- âœ… Test utilities scenarios
- âœ… Cross-environment testing

**Key Features**:
- Given-When-Then structure
- User-focused test descriptions
- Scenario-based testing
- Real-world usage patterns

## ğŸ› ï¸ Test Configuration

### Vitest Configuration (`vitest.config.mjs`)

```javascript
export default defineConfig({
  test: {
    globals: true,
    environment: 'node',
    timeout: 60000, // 60 seconds for Docker operations
    
    // Coverage configuration
    coverage: {
      provider: 'v8',
      reporter: ['text', 'json', 'html', 'lcov'],
      thresholds: {
        global: {
          branches: 80,
          functions: 80,
          lines: 80,
          statements: 80
        }
      }
    }
  }
})
```

### Coverage Thresholds

- **Branches**: 80%
- **Functions**: 80%
- **Lines**: 80%
- **Statements**: 80%

## ğŸ“Š Test Reports

### Coverage Reports
- **HTML**: `coverage/lcov-report/index.html`
- **JSON**: `coverage/coverage-final.json`
- **LCOV**: `coverage/lcov.info`

### Test Results
- **JSON**: `test-results.json`
- **HTML**: `test-results.html`

## ğŸ¯ Test Scenarios

### Unit Test Scenarios

#### Assertions API
- âœ… Exit code validation
- âœ… Output content matching (string/regex)
- âœ… Stderr validation
- âœ… JSON output handling
- âœ… Success/failure expectations
- âœ… Output length validation
- âœ… Method chaining

#### Scenario DSL
- âœ… Scenario builder creation
- âœ… Step definition and execution
- âœ… Expectation validation
- âœ… Error handling
- âœ… Test utilities (waitFor, retry, temp files)

#### Local Runner
- âœ… Project root detection
- âœ… Process spawning
- âœ… Error handling
- âœ… Timeout management
- âœ… JSON parsing
- âœ… Environment variables

### Integration Test Scenarios

#### Local Runner Integration
- âœ… GitVan CLI command execution
- âœ… Version command handling
- âœ… Invalid command handling
- âœ… Fluent assertions integration
- âœ… JSON output parsing
- âœ… Timeout handling

#### Cleanroom Runner Integration
- âœ… Docker container execution
- âœ… Multiple commands in same container
- âœ… Invalid command handling
- âœ… Fluent assertions in container
- âœ… Container lifecycle management

#### Scenario DSL Integration
- âœ… Complex multi-step scenarios
- âœ… Scenario failure handling
- âœ… Cross-runner compatibility

#### Test Utils Integration
- âœ… Temporary file creation/cleanup
- âœ… Retry logic for flaky operations
- âœ… Wait for conditions
- âœ… Complex workflow integration

### BDD Test Scenarios

#### CLI Help System
- âœ… User requests help
- âœ… Help includes all commands
- âœ… Help is well-formatted
- âœ… User requests version
- âœ… Version is valid semantic version

#### Error Handling
- âœ… Invalid command handling
- âœ… Helpful error messages
- âœ… Graceful failure (no crashes)

#### Command-Specific Help
- âœ… Help for specific commands
- âœ… Relevant help content
- âœ… Multiple command support

#### Docker Cleanroom Testing
- âœ… Isolated environment execution
- âœ… Consistent results
- âœ… Multiple commands in same container

#### Complex Workflows
- âœ… Multi-step GitVan workflows
- âœ… Reproducible workflows
- âœ… Scenario DSL integration

#### Test Utilities
- âœ… Temporary file management
- âœ… Retry logic for flaky operations
- âœ… Wait for conditions
- âœ… Cross-environment testing

## ğŸ”§ Test Utilities

### Available Utilities

```javascript
import { testUtils } from './index.js'

// Wait for conditions
await testUtils.waitFor(() => condition, timeout, interval)

// Retry operations
await testUtils.retry(operation, maxAttempts, delay)

// Temporary files
const tempFile = await testUtils.createTempFile(content, extension)
await testUtils.cleanupTempFiles([tempFile])
```

### Scenario DSL

```javascript
import { scenario } from './index.js'

const testScenario = scenario("Test Name")
  .step("Description")
  .run(args, options)
  .expect(result => result.expectSuccess())

const results = await testScenario.execute(runner)
```

## ğŸ³ Docker Requirements

For cleanroom tests, ensure Docker is running:

```bash
# Check Docker status
docker --version
docker ps

# Start Docker if needed
sudo systemctl start docker  # Linux
# or start Docker Desktop    # macOS/Windows
```

## ğŸ“ˆ Performance

### Test Execution Times
- **Unit Tests**: ~2-5 seconds
- **Integration Tests**: ~10-30 seconds
- **BDD Tests**: ~30-60 seconds
- **Full Suite**: ~60-120 seconds

### Optimization Tips
- Use `pnpm test:unit` for fast feedback during development
- Use `pnpm test:watch` for continuous testing
- Use `pnpm test:coverage` for coverage analysis
- Use `pnpm test:ui` for interactive debugging

## ğŸš¨ Troubleshooting

### Common Issues

#### Docker Not Running
```bash
# Error: Docker container failed to start
# Solution: Start Docker service
sudo systemctl start docker
```

#### Permission Issues
```bash
# Error: Permission denied
# Solution: Check file permissions
chmod +x run-tests.mjs
```

#### Timeout Issues
```bash
# Error: Test timeout
# Solution: Increase timeout in vitest.config.mjs
timeout: 120000  // 2 minutes
```

#### Coverage Issues
```bash
# Error: Coverage threshold not met
# Solution: Add more tests or adjust thresholds
```

## ğŸ“š Best Practices

### Writing Tests
1. **Unit Tests**: Mock external dependencies
2. **Integration Tests**: Use real components
3. **BDD Tests**: Focus on user scenarios
4. **Coverage**: Aim for 80%+ coverage
5. **Naming**: Use descriptive test names

### Test Organization
1. **Group related tests** in describe blocks
2. **Use beforeEach/afterEach** for setup/cleanup
3. **Keep tests independent** and isolated
4. **Use meaningful assertions** with clear error messages

### Performance
1. **Run unit tests frequently** during development
2. **Run integration tests** before commits
3. **Run BDD tests** for feature validation
4. **Use watch mode** for continuous testing

## ğŸ‰ Success Criteria

A successful test run should show:
- âœ… All unit tests passing
- âœ… All integration tests passing
- âœ… All BDD tests passing
- âœ… Coverage thresholds met (80%+)
- âœ… No flaky tests
- âœ… Clean test output

## ğŸ“ Support

For issues with the test suite:
1. Check the troubleshooting section
2. Review test output for specific errors
3. Ensure all dependencies are installed
4. Verify Docker is running for cleanroom tests
5. Check file permissions and paths
