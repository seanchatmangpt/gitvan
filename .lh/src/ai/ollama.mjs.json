{
    "sourceFile": "src/ai/ollama.mjs",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1758072770928,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1758072770928,
            "name": "Commit-0",
            "content": "/**\n * GitVan v2 Ollama AI Provider - Direct Ollama HTTP client\n * Provides integration with Ollama API for AI-powered job generation\n */\n\nconst BASE_URL = process.env.OLLAMA_BASE_URL || \"http://localhost:11434\";\n\n/**\n * Generate text using Ollama API\n * @param {object} options - Generation options\n * @param {string} options.model - Model name (default: qwen3-coder:30b)\n * @param {string} options.prompt - Input prompt\n * @param {object} options.options - Ollama options\n * @param {boolean} options.stream - Whether to stream response\n * @returns {Promise<string>} Generated text\n */\nexport async function generate({ \n  model = \"qwen3-coder:30b\", \n  prompt, \n  options = {}, \n  stream = false \n}) {\n  const requestBody = {\n    model,\n    prompt,\n    stream,\n    options: {\n      temperature: 0.7,\n      top_p: 0.8,\n      top_k: 20,\n      repeat_penalty: 1.05,\n      ...options\n    }\n  };\n\n  try {\n    const response = await fetch(`${BASE_URL}/api/generate`, {\n      method: \"POST\",\n      headers: { \n        \"Content-Type\": \"application/json\",\n        \"Accept\": \"application/json\"\n      },\n      body: JSON.stringify(requestBody)\n    });\n\n    if (!response.ok) {\n      throw new Error(`Ollama API error: ${response.status} ${response.statusText}`);\n    }\n\n    const data = await response.json();\n    return data.response || \"\";\n  } catch (error) {\n    throw new Error(`Failed to generate text with Ollama: ${error.message}`);\n  }\n}\n\n/**\n * Generate embeddings using Ollama API\n * @param {object} options - Embedding options\n * @param {string} options.model - Model name\n * @param {string} options.text - Text to embed\n * @returns {Promise<Array<number>>} Embedding vector\n */\nexport async function embed({ model = \"qwen3-coder:30b\", text }) {\n  try {\n    const response = await fetch(`${BASE_URL}/api/embeddings`, {\n      method: \"POST\",\n      headers: { \"Content-Type\": \"application/json\" },\n      body: JSON.stringify({ model, prompt: text })\n    });\n\n    if (!response.ok) {\n      throw new Error(`Ollama embeddings error: ${response.status}`);\n    }\n\n    const data = await response.json();\n    return data.embedding || [];\n  } catch (error) {\n    throw new Error(`Failed to generate embeddings: ${error.message}`);\n  }\n}\n\n/**\n * Check if Ollama is available and model is loaded\n * @param {string} model - Model name to check\n * @returns {Promise<boolean>} True if model is available\n */\nexport async function checkModel(model = \"qwen3-coder:30b\") {\n  try {\n    const response = await fetch(`${BASE_URL}/api/tags`);\n    if (!response.ok) return false;\n    \n    const data = await response.json();\n    return data.models?.some(m => m.name === model) || false;\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Pull/download a model from Ollama\n * @param {string} model - Model name to pull\n * @returns {Promise<void>}\n */\nexport async function pullModel(model = \"qwen3-coder:30b\") {\n  try {\n    const response = await fetch(`${BASE_URL}/api/pull`, {\n      method: \"POST\",\n      headers: { \"Content-Type\": \"application/json\" },\n      body: JSON.stringify({ name: model, stream: false })\n    });\n\n    if (!response.ok) {\n      throw new Error(`Failed to pull model: ${response.status}`);\n    }\n  } catch (error) {\n    throw new Error(`Failed to pull model ${model}: ${error.message}`);\n  }\n}\n\n"
        }
    ]
}