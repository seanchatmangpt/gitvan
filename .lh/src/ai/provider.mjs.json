{
    "sourceFile": "src/ai/provider.mjs",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1758057329097,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1758057329097,
            "name": "Commit-0",
            "content": "/**\n * GitVan v2 AI Provider - Unified AI provider interface\n * Supports Ollama (default), HTTP providers, and future Vercel AI SDK integration\n */\n\nimport {\n  generate as ollamaGenerate,\n  embed as ollamaEmbed,\n  checkModel,\n  pullModel,\n} from \"./ollama.mjs\";\nimport { createLogger } from \"../utils/logger.mjs\";\nimport { sha256Hex, fingerprint } from \"../utils/crypto.mjs\";\n\nconst logger = createLogger(\"ai\");\n\n/**\n * Generate text using configured AI provider\n * @param {object} options - Generation options\n * @param {string} options.prompt - Input prompt\n * @param {string} options.model - Model name (default: qwen3-coder:30b)\n * @param {object} options.options - Provider-specific options\n * @param {object} options.config - GitVan config\n * @returns {Promise<object>} Generation result with metadata\n */\nexport async function generateText({\n  prompt,\n  model,\n  options = {},\n  config = {},\n}) {\n  const aiConfig = config.ai || {};\n  const provider = aiConfig.provider || \"ollama\";\n  const defaultModel = aiConfig.model || \"qwen3-coder:30b\";\n  const finalModel = model || defaultModel;\n\n  // Default options optimized for qwen3-coder:30b\n  const defaultOptions = {\n    temperature: 0.7,\n    top_p: 0.8,\n    top_k: 20,\n    repeat_penalty: 1.05,\n    max_tokens: 4096,\n    ...aiConfig.defaults,\n    ...options,\n  };\n\n  const startTime = Date.now();\n\n  try {\n    let response;\n\n    switch (provider) {\n      case \"ollama\":\n        response = await ollamaGenerate({\n          model: finalModel,\n          prompt,\n          options: defaultOptions,\n        });\n        break;\n\n      case \"http\":\n        response = await generateWithHTTP({\n          model: finalModel,\n          prompt,\n          options: defaultOptions,\n          config: aiConfig.http,\n        });\n        break;\n\n      default:\n        throw new Error(`Unsupported AI provider: ${provider}`);\n    }\n\n    const duration = Date.now() - startTime;\n\n    return {\n      output: response,\n      model: finalModel,\n      provider,\n      options: defaultOptions,\n      duration,\n      promptHash: sha256Hex(prompt),\n      outputHash: sha256Hex(response),\n      fingerprint: fingerprint({\n        model: finalModel,\n        prompt: prompt,\n        options: defaultOptions,\n      }),\n    };\n  } catch (error) {\n    logger.error(\"AI generation failed:\", error.message);\n    throw error;\n  }\n}\n\n/**\n * Generate embeddings using configured AI provider\n * @param {object} options - Embedding options\n * @param {string} options.text - Text to embed\n * @param {string} options.model - Model name\n * @param {object} options.config - GitVan config\n * @returns {Promise<object>} Embedding result with metadata\n */\nexport async function generateEmbeddings({ text, model, config = {} }) {\n  const aiConfig = config.ai || {};\n  const provider = aiConfig.provider || \"ollama\";\n  const defaultModel = aiConfig.model || \"qwen3-coder:30b\";\n  const finalModel = model || defaultModel;\n\n  const startTime = Date.now();\n\n  try {\n    let embedding;\n\n    switch (provider) {\n      case \"ollama\":\n        embedding = await ollamaEmbed({\n          model: finalModel,\n          text,\n        });\n        break;\n\n      case \"http\":\n        embedding = await embedWithHTTP({\n          model: finalModel,\n          text,\n          config: aiConfig.http,\n        });\n        break;\n\n      default:\n        throw new Error(`Unsupported AI provider: ${provider}`);\n    }\n\n    const duration = Date.now() - startTime;\n\n    return {\n      embedding,\n      model: finalModel,\n      provider,\n      dimensions: embedding.length,\n      duration,\n      textHash: sha256Hex(text),\n    };\n  } catch (error) {\n    logger.error(\"AI embedding failed:\", error.message);\n    throw error;\n  }\n}\n\n/**\n * Check if AI provider and model are available\n * @param {object} config - GitVan config\n * @returns {Promise<object>} Availability status\n */\nexport async function checkAIAvailability(config = {}) {\n  const aiConfig = config.ai || {};\n  const provider = aiConfig.provider || \"ollama\";\n  const model = aiConfig.model || \"qwen3-coder:30b\";\n\n  try {\n    switch (provider) {\n      case \"ollama\":\n        const isAvailable = await checkModel(model);\n        return {\n          available: isAvailable,\n          provider,\n          model,\n          message: isAvailable\n            ? \"Ollama model available\"\n            : `Model ${model} not found in Ollama`,\n        };\n\n      case \"http\":\n        return {\n          available: true,\n          provider,\n          model,\n          message: \"HTTP provider configured\",\n        };\n\n      default:\n        return {\n          available: false,\n          provider,\n          model,\n          message: `Unknown provider: ${provider}`,\n        };\n    }\n  } catch (error) {\n    return {\n      available: false,\n      provider,\n      model,\n      message: `Provider check failed: ${error.message}`,\n    };\n  }\n}\n\n/**\n * Pull/download model for Ollama provider\n * @param {string} model - Model name\n * @returns {Promise<void>}\n */\nexport async function ensureModel(model = \"qwen3-coder:30b\") {\n  try {\n    await pullModel(model);\n    logger.info(`Model ${model} pulled successfully`);\n  } catch (error) {\n    logger.error(`Failed to pull model ${model}:`, error.message);\n    throw error;\n  }\n}\n\n// HTTP provider implementations (placeholder)\nasync function generateWithHTTP({ model, prompt, options, config }) {\n  // Implementation for generic HTTP AI providers\n  throw new Error(\"HTTP provider not yet implemented\");\n}\n\nasync function embedWithHTTP({ model, text, config }) {\n  // Implementation for generic HTTP embedding providers\n  throw new Error(\"HTTP embedding provider not yet implemented\");\n}\n"
        }
    ]
}